#!/usr/bin/env python3
"""
–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ RAG —Å –∞–≥–µ–Ω—Ç–∞–º–∏
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø–æ–∏—Å–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∞–≥–µ–Ω—Ç–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º
"""

import os
import json
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
from dotenv import load_dotenv
load_dotenv()

# LangChain imports
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS, Chroma
from langchain.chains import RetrievalQA
from langchain.agents import initialize_agent, Tool, AgentType
from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory
from langchain.schema import Document
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.callbacks import StdOutCallbackHandler

# OpenRouter integration
class ChatOpenRouter(ChatOpenAI):
    """
    OpenRouter –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è LangChain
    –°–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å OpenAI API –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –º–æ–¥–µ–ª—è–º
    """
    def __init__(self, model_name: str = "anthropic/claude-3.5-sonnet", **kwargs):
        # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        openrouter_api_key = os.getenv('OPENROUTER_API_KEY')
        if not openrouter_api_key:
            raise ValueError("OPENROUTER_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–ª–∞—Å—Å —Å OpenRouter –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        super().__init__(
            model=model_name,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º model –≤–º–µ—Å—Ç–æ model_name –¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π
            openai_api_key=openrouter_api_key,
            openai_api_base="https://openrouter.ai/api/v1",
            default_headers={
                "HTTP-Referer": "https://github.com/your-repo",  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                "X-Title": "RAG Agent Examples",  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            },
            **kwargs
        )

class OpenRouterEmbeddings(OpenAIEmbeddings):
    """
    OpenRouter —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º OpenAI —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ OpenRouter)
    """
    def __init__(self, model: str = "openai/text-embedding-ada-002", **kwargs):
        openrouter_api_key = os.getenv('OPENROUTER_API_KEY')
        if not openrouter_api_key:
            # Fallback –∫ OpenAI –µ—Å–ª–∏ OpenRouter –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω
            openrouter_api_key = os.getenv('OPENAI_API_KEY')
            if not openrouter_api_key:
                raise ValueError("–ù–∏ OPENROUTER_API_KEY, –Ω–∏ OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        super().__init__(
            model=model,
            openai_api_key=openrouter_api_key,
            openai_api_base="https://openrouter.ai/api/v1",
            **kwargs
        )

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity


@dataclass
class AgentConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è RAG-–∞–≥–µ–Ω—Ç–∞"""
    # –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
    model_name: str = "gpt-4o-mini"  # OpenAI –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    openrouter_model: str = "anthropic/claude-3.5-sonnet"  # OpenRouter –º–æ–¥–µ–ª—å
    embedding_model: str = "text-embedding-ada-002"  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ OpenAI
    openrouter_embedding_model: str = "openai/text-embedding-ada-002"  # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ OpenRouter
    temperature: float = 0.1
    max_tokens: int = 2000
    chunk_size: int = 1000
    chunk_overlap: int = 200
    retrieval_k: int = 4
    similarity_threshold: float = 0.7
    use_openrouter: bool = False  # –§–ª–∞–≥ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞


class MultiSourceRAGAgent:
    """
    –ê–≥–µ–Ω—Ç —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –∑–Ω–∞–Ω–∏–π –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º
    """
    
    def __init__(self, config: AgentConfig = None):
        self.config = config or AgentConfig()
        self.knowledge_bases = {}
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if self.config.use_openrouter:
            self.llm = ChatOpenRouter(
                model_name=self.config.openrouter_model,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                request_timeout=30,
                max_retries=1
            )
        else:
            self.llm = ChatOpenAI(
                model=self.config.model_name,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                request_timeout=30,
                max_retries=1
            )
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –ø–∞–º—è—Ç—å –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è deprecation warnings
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        self.conversation_history = []
        
    def add_knowledge_base(self, name: str, documents_path: str, description: str):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∑–Ω–∞–Ω–∏–π"""
        print(f"–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {name}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if os.path.isfile(documents_path):
            if documents_path.endswith('.pdf'):
                loader = PyPDFLoader(documents_path)
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {documents_path}")
        else:
            loader = DirectoryLoader(documents_path, glob="**/*.md")
            
        documents = loader.load()
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=["\n\n", "\n", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        for chunk in chunks:
            chunk.metadata['knowledge_base'] = name
            chunk.metadata['timestamp'] = datetime.now().isoformat()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        if self.config.use_openrouter:
            embeddings = OpenRouterEmbeddings(model=self.config.openrouter_embedding_model)
        else:
            embeddings = OpenAIEmbeddings(model=self.config.embedding_model)
        
        vectorstore = FAISS.from_documents(chunks, embeddings)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ QA —Ü–µ–ø–æ—á–∫–∏
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(
                search_kwargs={"k": self.config.retrieval_k}
            ),
            return_source_documents=True
        )
        
        self.knowledge_bases[name] = {
            "qa_chain": qa_chain,
            "vectorstore": vectorstore,
            "description": description,
            "document_count": len(chunks)
        }
        
        print(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π '{name}' —Å–æ–∑–¥–∞–Ω–∞: {len(chunks)} —á–∞–Ω–∫–æ–≤")
    
    def classify_query(self, query: str) -> List[str]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        classification_prompt = f"""
        –û–ø—Ä–µ–¥–µ–ª–∏, –∫–∞–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–Ω–∞–Ω–∏–π –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
        
        –î–æ—Å—Ç—É–ø–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:
        {chr(10).join([f"- {name}: {info['description']}" for name, info in self.knowledge_bases.items()])}
        
        –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}
        
        –í–µ—Ä–Ω–∏ —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é.
        –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω, –≤–µ—Ä–Ω–∏ –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.
        """
        
        try:
            llm_response = self.llm.invoke(classification_prompt)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –æ—Ç–≤–µ—Ç–∞
            if hasattr(llm_response, 'content'):
                response = llm_response.content
            else:
                response = str(llm_response)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return list(self.knowledge_bases.keys())  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞
        suggested_sources = [s.strip() for s in response.split(",")]
        valid_sources = [s for s in suggested_sources if s in self.knowledge_bases]
        
        return valid_sources if valid_sources else list(self.knowledge_bases.keys())
    
    def search_knowledge_bases(self, query: str, sources: List[str] = None) -> Dict[str, Any]:
        """–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
        if sources is None:
            sources = self.classify_query(query)
        
        results = {}
        all_documents = []
        
        for source_name in sources:
            if source_name in self.knowledge_bases:
                kb = self.knowledge_bases[source_name]
                result = kb["qa_chain"].invoke({"query": query})
                
                results[source_name] = {
                    "answer": result["result"],
                    "source_documents": result["source_documents"],
                    "confidence": self._calculate_confidence(result["source_documents"], query)
                }
                all_documents.extend(result["source_documents"])
        
        return {
            "source_results": results,
            "all_documents": all_documents,
            "sources_used": sources
        }
    
    def _calculate_confidence(self, documents: List[Document], query: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∑–∞–ø—Ä–æ—Å–æ–º"""
        if not documents:
            return 0.0
        
        try:
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            if self.config.use_openrouter:
                embeddings = OpenRouterEmbeddings(model=self.config.embedding_model)
            else:
                embeddings = OpenAIEmbeddings(model=self.config.embedding_model)
            
            query_embedding = embeddings.embed_query(query)
            
            similarities = []
            for doc in documents:
                doc_embedding = embeddings.embed_documents([doc.page_content])[0]
                similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]
                similarities.append(similarity)
            
            return float(np.mean(similarities)) if similarities else 0.0
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {e}")
            return 0.5  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω—é—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
    
    def synthesize_answer(self, query: str, search_results: Dict[str, Any]) -> str:
        """–°–∏–Ω—Ç–µ–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        source_answers = search_results["source_results"]
        
        if not source_answers:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."
        
        if len(source_answers) == 1:
            # –û–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ –æ—Ç–≤–µ—Ç
            return list(source_answers.values())[0]["answer"]
        
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ - —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        synthesis_context = []
        for source_name, result in source_answers.items():
            synthesis_context.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫ '{source_name}': {result['answer']}")
        
        synthesis_prompt = f"""
        –ù–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–π—Ç–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å.
        
        –í–æ–ø—Ä–æ—Å: {query}
        
        –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:
        {chr(10).join(synthesis_context)}
        
        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
        1. –û–±—ä–µ–¥–∏–Ω–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        2. –£–∫–∞–∂–∏—Ç–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        3. –°–¥–µ–ª–∞–π—Ç–µ –æ—Ç–≤–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ –ø–æ–Ω—è—Ç–Ω—ã–º
        4. –£–∫–∞–∂–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
        
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç–≤–µ—Ç:
        """
        
        llm_response = self.llm.invoke(synthesis_prompt)
        if hasattr(llm_response, 'content'):
            return llm_response.content
        else:
            return str(llm_response)
    
    def chat(self, message: str) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—â–µ–Ω–∏—è —Å –∞–≥–µ–Ω—Ç–æ–º"""
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.conversation_history.append({
            "timestamp": datetime.now(),
            "user_message": message,
            "type": "user"
        })
        
        # –ü–æ–∏—Å–∫ –≤ –±–∞–∑–∞—Ö –∑–Ω–∞–Ω–∏–π
        search_results = self.search_knowledge_bases(message)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM
        context = self._format_context(message, search_results)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å —É—á—ë—Ç–æ–º –ø–∞–º—è—Ç–∏
        response_prompt = f"""
        –¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –±–∞–∑–∞–º –∑–Ω–∞–Ω–∏–π.
        
        –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å):
        {self._get_conversation_context()}
        
        –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑ –∑–Ω–∞–Ω–∏–π:
        {context}
        
        –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {message}
        
        –î–∞–π –ø–æ–ª–Ω—ã–π –∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
        –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Å—Ç–Ω–æ –æ–± —ç—Ç–æ–º —Å–∫–∞–∂–∏.
        """
        
        try:
            llm_response = self.llm.invoke(response_prompt)
            if hasattr(llm_response, 'content'):
                response = llm_response.content
            else:
                response = str(llm_response)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.conversation_history.append({
            "timestamp": datetime.now(),
            "assistant_response": response,
            "sources_used": search_results["sources_used"],
            "type": "assistant"
        })
        
        return response
    
    def _format_context(self, query: str, search_results: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        if not search_results["source_results"]:
            return "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."
        
        formatted_context = []
        
        for source_name, result in search_results["source_results"].items():
            formatted_context.append(f"\n--- –ò—Å—Ç–æ—á–Ω–∏–∫: {source_name} ---")
            formatted_context.append(f"–û—Ç–≤–µ—Ç: {result['answer']}")
            formatted_context.append(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2f}")
            
            if result["source_documents"]:
                formatted_context.append("–î–æ–∫—É–º–µ–Ω—Ç—ã:")
                for i, doc in enumerate(result["source_documents"][:2]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 2
                    formatted_context.append(f"  {i+1}. {doc.page_content[:200]}...")
        
        return "\n".join(formatted_context)
    
    def _get_conversation_context(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
        if len(self.conversation_history) <= 2:
            return "–ù–∞—á–∞–ª–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞."
        
        recent_messages = self.conversation_history[-6:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –ø–∞—Ä—ã
        context = []
        
        for msg in recent_messages:
            if msg["type"] == "user":
                context.append(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {msg['user_message']}")
            else:
                context.append(f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {msg['assistant_response'][:100]}...")
        
        return "\n".join(context)
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞"""
        stats = {
            "knowledge_bases": len(self.knowledge_bases),
            "total_documents": sum(kb["document_count"] for kb in self.knowledge_bases.values()),
            "conversation_turns": len([msg for msg in self.conversation_history if msg["type"] == "user"]),
            "knowledge_base_details": {}
        }
        
        for name, kb in self.knowledge_bases.items():
            stats["knowledge_base_details"][name] = {
                "description": kb["description"],
                "document_count": kb["document_count"]
            }
        
        return stats


class SmartRetrievalAgent:
    """
    –ê–≥–µ–Ω—Ç —Å —É–º–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –ø–æ–∏—Å–∫–∞ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º
    """
    
    def __init__(self, config: AgentConfig = None):
        self.config = config or AgentConfig()
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        if self.config.use_openrouter:
            self.llm = ChatOpenRouter(
                model_name=self.config.openrouter_model,
                temperature=0,
                request_timeout=30,
                max_retries=1
            )
        else:
            self.llm = ChatOpenAI(
                model=self.config.model_name,
                temperature=0,
                request_timeout=30,
                max_retries=1
            )
        self.vectorstore = None
        self.feedback_history = []
        self.query_success_rate = {}
        
    def setup_knowledge_base(self, documents_path: str):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        loader = DirectoryLoader(documents_path, glob="**/*.md")
        documents = loader.load()
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        chunk_size = self._determine_optimal_chunk_size(documents)
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=int(chunk_size * 0.2),
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        
        # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        for i, chunk in enumerate(chunks):
            chunk.metadata.update({
                "chunk_id": f"chunk_{i}",
                "doc_type": self._classify_document_type(chunk.page_content),
                "complexity_score": self._calculate_complexity_score(chunk.page_content),
                "created_at": datetime.now().isoformat()
            })
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        if self.config.use_openrouter:
            embeddings = OpenRouterEmbeddings(model=self.config.openrouter_embedding_model)
        else:
            embeddings = OpenAIEmbeddings(model=self.config.embedding_model)
        
        self.vectorstore = FAISS.from_documents(chunks, embeddings)
        
        print(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞: {len(chunks)} —á–∞–Ω–∫–æ–≤ —Å —Ä–∞–∑–º–µ—Ä–æ–º {chunk_size}")
    
    def _determine_optimal_chunk_size(self, documents: List[Document]) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        doc_lengths = [len(doc.page_content) for doc in documents]
        avg_length = np.mean(doc_lengths)
        
        if avg_length < 2000:
            return 500  # –ö–æ—Ä–æ—Ç–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        elif avg_length < 10000:
            return 1000  # –°—Ä–µ–¥–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        else:
            return 1500  # –î–ª–∏–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    
    def _classify_document_type(self, content: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        content_lower = content.lower()
        
        if "class " in content_lower or "def " in content_lower:
            return "code"
        elif "api" in content_lower or "endpoint" in content_lower:
            return "api_docs"
        elif "tutorial" in content_lower or "step" in content_lower:
            return "tutorial"
        else:
            return "general"
    
    def _calculate_complexity_score(self, content: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        sentences = content.split('.')
        avg_sentence_length = np.mean([len(s.split()) for s in sentences if s.strip()])
        
        technical_terms = ["api", "function", "class", "method", "parameter", "algorithm"]
        tech_term_count = sum(1 for term in technical_terms if term in content.lower())
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ—Ç 0 –¥–æ 1
        complexity = min(1.0, (avg_sentence_length / 20) + (tech_term_count / len(technical_terms)))
        return complexity
    
    def intelligent_search(self, query: str, user_level: str = "intermediate") -> Dict[str, Any]:
        """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É—á—ë—Ç–æ–º —É—Ä–æ–≤–Ω—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if not self.vectorstore:
            return {"error": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞"}
        
        # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
        query_complexity = self._analyze_query_complexity(query)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        k = self._determine_retrieval_k(query_complexity, user_level)
        
        # –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        complexity_filter = self._get_complexity_filter(user_level)
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞
        docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=k*2)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        filtered_docs = []
        for doc, score in docs_with_scores:
            doc_complexity = doc.metadata.get("complexity_score", 0.5)
            if self._is_suitable_complexity(doc_complexity, complexity_filter):
                filtered_docs.append((doc, score))
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        final_docs = filtered_docs[:k]
        
        if not final_docs:
            return {
                "answer": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∞—à–µ–≥–æ —É—Ä–æ–≤–Ω—è.",
                "documents": [],
                "suggestion": "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏."
            }
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ—Ç–≤–µ—Ç–∞
        context = self._build_adaptive_context(final_docs, user_level)
        answer = self._generate_adaptive_answer(query, context, user_level)
        
        return {
            "answer": answer,
            "documents": [doc for doc, _ in final_docs],
            "relevance_scores": [float(score) for _, score in final_docs],
            "query_complexity": query_complexity,
            "user_level": user_level
        }
    
    def _analyze_query_complexity(self, query: str) -> float:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        # –ü—Ä–æ—Å—Ç—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        complex_words = ["implement", "architecture", "algorithm", "optimization", "advanced"]
        simple_words = ["what", "how", "example", "basic", "simple"]
        
        query_lower = query.lower()
        complexity_score = 0.5  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        
        for word in complex_words:
            if word in query_lower:
                complexity_score += 0.1
        
        for word in simple_words:
            if word in query_lower:
                complexity_score -= 0.1
        
        return max(0.1, min(1.0, complexity_score))
    
    def _determine_retrieval_k(self, query_complexity: float, user_level: str) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        base_k = {"beginner": 2, "intermediate": 4, "advanced": 6}
        k = base_k.get(user_level, 4)
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º k –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if query_complexity > 0.7:
            k += 2
        
        return k
    
    def _get_complexity_filter(self, user_level: str) -> tuple:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —É—Ä–æ–≤–Ω—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        filters = {
            "beginner": (0.0, 0.4),
            "intermediate": (0.2, 0.8),
            "advanced": (0.6, 1.0)
        }
        return filters.get(user_level, (0.0, 1.0))
    
    def _is_suitable_complexity(self, doc_complexity: float, complexity_filter: tuple) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥—Ö–æ–¥—è—â–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        min_complexity, max_complexity = complexity_filter
        return min_complexity <= doc_complexity <= max_complexity
    
    def _build_adaptive_context(self, docs_with_scores: List[tuple], user_level: str) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        context_parts = []
        
        for i, (doc, score) in enumerate(docs_with_scores):
            doc_type = doc.metadata.get("doc_type", "general")
            complexity = doc.metadata.get("complexity_score", 0.5)
            
            context_parts.append(f"--- –î–æ–∫—É–º–µ–Ω—Ç {i+1} (—Ç–∏–ø: {doc_type}, —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {complexity:.1f}) ---")
            
            # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π
            if user_level == "beginner":
                content = doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
            elif user_level == "intermediate":
                content = doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
            else:  # advanced
                content = doc.page_content
            
            context_parts.append(content)
        
        return "\n\n".join(context_parts)
    
    def _generate_adaptive_answer(self, query: str, context: str, user_level: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        level_instructions = {
            "beginner": "–û–±—ä—è—Å–Ω–∏ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –∏—Å–ø–æ–ª—å–∑—É–π –∞–Ω–∞–ª–æ–≥–∏–∏, –∏–∑–±–µ–≥–∞–π —Å–ª–æ–∂–Ω–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏",
            "intermediate": "–î–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é —É–º–µ—Ä–µ–Ω–Ω–æ",
            "advanced": "–î–∞–π –≥–ª—É–±–æ–∫–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, –≤–∫–ª—é—á–∏ –¥–µ—Ç–∞–ª–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ best practices"
        }
        
        instruction = level_instructions.get(user_level, level_instructions["intermediate"])
        
        prompt = f"""
        {instruction}.
        
        –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:
        {context}
        
        –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—É—Ä–æ–≤–µ–Ω—å {user_level}): {query}
        
        –û—Ç–≤–µ—Ç:
        """
        
        llm_response = self.llm.invoke(prompt)
        if hasattr(llm_response, 'content'):
            return llm_response.content
        else:
            return str(llm_response)
    
    def learn_from_feedback(self, query: str, answer: str, feedback_score: int, comments: str = ""):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
        feedback_entry = {
            "query": query,
            "answer": answer,
            "score": feedback_score,  # 1-5
            "comments": comments,
            "timestamp": datetime.now().isoformat()
        }
        
        self.feedback_history.append(feedback_entry)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        query_type = self._classify_query_type(query)
        if query_type not in self.query_success_rate:
            self.query_success_rate[query_type] = []
        
        self.query_success_rate[query_type].append(feedback_score >= 4)  # 4-5 —Å—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º–∏
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä)
        if feedback_score < 3:
            print(f"–ù–∏–∑–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —Ç–∏–ø–∞ '{query_type}'. –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è.")
    
    def _classify_query_type(self, query: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["how to", "tutorial", "guide"]):
            return "how-to"
        elif any(word in query_lower for word in ["what is", "define", "explain"]):
            return "definition"
        elif any(word in query_lower for word in ["error", "bug", "problem", "fix"]):
            return "troubleshooting"
        elif any(word in query_lower for word in ["example", "sample", "demo"]):
            return "example"
        else:
            return "general"
    
    def get_learning_analytics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        if not self.feedback_history:
            return {"message": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_feedback = len(self.feedback_history)
        avg_score = np.mean([f["score"] for f in self.feedback_history])
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤
        type_stats = {}
        for query_type, success_rates in self.query_success_rate.items():
            type_stats[query_type] = {
                "total_queries": len(success_rates),
                "success_rate": np.mean(success_rates) * 100,
                "success_count": sum(success_rates)
            }
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–∏–∑–∫–∏–µ –æ—Ü–µ–Ω–∫–∏
        low_scores = [f for f in self.feedback_history[-10:] if f["score"] < 3]
        
        return {
            "total_feedback": total_feedback,
            "average_score": round(avg_score, 2),
            "query_type_performance": type_stats,
            "recent_low_scores": low_scores,
            "improvement_suggestions": self._generate_improvement_suggestions()
        }
    
    def _generate_improvement_suggestions(self) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        suggestions = []
        
        for query_type, success_rates in self.query_success_rate.items():
            success_rate = np.mean(success_rates) * 100
            if success_rate < 70:
                suggestions.append(f"–£–ª—É—á—à–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ '{query_type}' (—É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}%)")
        
        return suggestions


def demo_multi_source_agent():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏"""
    print("=== –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è MultiSourceRAGAgent —Å OpenRouter ===")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–µ–π
    openrouter_key = os.getenv("OPENROUTER_API_KEY")
    openai_key = os.getenv("OPENAI_API_KEY")
    
    if not openrouter_key and not openai_key:
        print("‚ö†Ô∏è  –ù–∏ OPENROUTER_API_KEY, –Ω–∏ OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã.")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∏–∑ –Ω–∏—Ö –≤ .env —Ñ–∞–π–ª–µ –¥–ª—è –ø–æ–ª–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.")
        return
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
    # –í—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º OpenAI –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    use_openrouter = False  # bool(openrouter_key)
    provider_name = "OpenRouter" if use_openrouter else "OpenAI"
    print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider_name}")
    
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        config = AgentConfig(use_openrouter=use_openrouter)
        agent = MultiSourceRAGAgent(config)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–Ω–∞–Ω–∏–π
        print("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑ –∑–Ω–∞–Ω–∏–π...")
        
        knowledge_base_path = os.path.join(os.path.dirname(__file__), "knowledge_base")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏
        if not os.path.exists(knowledge_base_path):
            print(f"‚ùå –ü–∞–ø–∫–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {knowledge_base_path}")
            print("–°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É knowledge_base —Å –ø–æ–¥–ø–∞–ø–∫–∞–º–∏ technical_docs, api_reference, tutorials")
            return
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–Ω–∞–Ω–∏–π
        sources = [
            ("technical_docs", os.path.join(knowledge_base_path, "technical_docs"), 
             "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ë–î –∏ RAG –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ"),
            ("api_reference", os.path.join(knowledge_base_path, "api_reference"), 
             "API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è LangChain, FAISS –∏ ChromaDB"),
            ("tutorials", os.path.join(knowledge_base_path, "tutorials"), 
             "–ü–æ—à–∞–≥–æ–≤—ã–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏")
        ]
        
        for name, path, description in sources:
            if os.path.exists(path) and os.listdir(path):
                agent.add_knowledge_base(name, path, description)
            else:
                print(f"‚ö†Ô∏è  –ü–∞–ø–∫–∞ {path} –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        
        # –†–µ–∞–ª—å–Ω—ã–π –¥–∏–∞–ª–æ–≥ —Å –∞–≥–µ–Ω—Ç–æ–º (—Å–æ–∫—Ä–∞—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)
        queries = [
            "–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å FAISS –∏–Ω–¥–µ–∫—Å?",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ ChromaDB?",
            "–û—Å–Ω–æ–≤—ã RAG –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"
        ]
        
        for query in queries:
            print(f"\nü§î –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {query}")
            print("=" * 60)
            
            try:
                import signal
                import time
                
                def timeout_handler(signum, frame):
                    raise TimeoutError("–ó–∞–ø—Ä–æ—Å –ø—Ä–µ–≤—ã—Å–∏–ª –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è")
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –Ω–∞ 45 —Å–µ–∫—É–Ω–¥
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(45)
                
                start_time = time.time()
                response = agent.chat(query)
                elapsed_time = time.time() - start_time
                
                signal.alarm(0)  # –û—Ç–∫–ª—é—á–∞–µ–º —Ç–∞–π–º–∞—É—Ç
                
                print(f"ü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç ({elapsed_time:.1f}—Å): {response}")
                
            except TimeoutError:
                print(f"‚è∞ –ó–∞–ø—Ä–æ—Å –ø—Ä–µ–≤—ã—Å–∏–ª –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è (45—Å). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                signal.alarm(0)
            except KeyboardInterrupt:
                print(f"‚èπÔ∏è  –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–µ—Ä–≤–∞–ª –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ")
                signal.alarm(0)
                break
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}")
                signal.alarm(0)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = agent.get_statistics()
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞:")
        print(f"- –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–Ω–∞–Ω–∏–π: {stats['knowledge_bases']}")
        print(f"- –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['total_documents']}")
        print(f"- –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['conversation_turns']}")
        
        print("\nüìö –î–µ—Ç–∞–ª–∏ –±–∞–∑ –∑–Ω–∞–Ω–∏–π:")
        for name, details in stats['knowledge_base_details'].items():
            print(f"  ‚Ä¢ {name}: {details['document_count']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            print(f"    –û–ø–∏—Å–∞–Ω–∏–µ: {details['description']}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞–≥–µ–Ω—Ç–∞: {e}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω OPENAI_API_KEY")


def demo_smart_retrieval_agent():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —É–º–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –ø–æ–∏—Å–∫–∞ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    print("\n=== –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è SmartRetrievalAgent —Å OpenRouter ===")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–µ–π
    openrouter_key = os.getenv("OPENROUTER_API_KEY")
    openai_key = os.getenv("OPENAI_API_KEY")
    
    if not openrouter_key and not openai_key:
        print("‚ö†Ô∏è  –ù–∏ OPENROUTER_API_KEY, –Ω–∏ OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã.")
        return
    
    # –í—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º OpenAI –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    use_openrouter = False  # bool(openrouter_key)
    provider_name = "OpenRouter" if use_openrouter else "OpenAI"
    print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider_name}")
    
    try:
        config = AgentConfig(use_openrouter=use_openrouter)
        agent = SmartRetrievalAgent(config)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        knowledge_base_path = os.path.join(os.path.dirname(__file__), "knowledge_base")
        
        if os.path.exists(knowledge_base_path):
            agent.setup_knowledge_base(knowledge_base_path)
            print("‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
        else:
            print("‚ùå –ü–∞–ø–∫–∞ knowledge_base –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–æ–∑–¥–∞–π—Ç–µ –µ—ë —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.")
            return
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        test_queries = [
            ("–ß—Ç–æ —Ç–∞–∫–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö?", "beginner"),
            ("–ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å FAISS –∏–Ω–¥–µ–∫—Å –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏?", "intermediate"),
            ("–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—Ä–æ—Å—Å-—ç–Ω–∫–æ–¥–µ—Ä–æ–≤", "advanced"),
            ("–û–±—ä—è—Å–Ω–∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã RAG –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "intermediate")
        ]
        
        for query, level in test_queries:
            print(f"\nüéØ –ó–∞–ø—Ä–æ—Å ({level}): {query}")
            print("=" * 70)
            
            try:
                result = agent.intelligent_search(query, level)
                
                if 'error' in result:
                    print(f"‚ùå {result['error']}")
                else:
                    print(f"üìù –û—Ç–≤–µ—Ç: {result['answer'][:300]}...")
                    print(f"üìä –°–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞: {result['query_complexity']:.2f}")
                    print(f"üë§ –£—Ä–æ–≤–µ–Ω—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {result['user_level']}")
                    print(f"üìö –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(result['documents'])}")
                    
                    if result['relevance_scores']:
                        avg_relevance = sum(result['relevance_scores']) / len(result['relevance_scores'])
                        print(f"üéØ –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {avg_relevance:.3f}")
                        
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
        print("\nüß† –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏:")
        print("=" * 50)
        
        feedback_examples = [
            ("–ß—Ç–æ —Ç–∞–∫–æ–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏?", "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ - —ç—Ç–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞...", 5, "–û—Ç–ª–∏—á–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ!"),
            ("–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç HNSW –∞–ª–≥–æ—Ä–∏—Ç–º?", "HNSW —ç—Ç–æ –≥—Ä–∞—Ñ...", 4, "–•–æ—Ä–æ—à–æ, –Ω–æ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π"),
            ("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ ChromaDB", "–ù–µ–ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç", 2, "–ù—É–∂–Ω–æ –±–æ–ª—å—à–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤"),
            ("API LangChain", "–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∫–æ–¥–∞", 5, "–û—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω–æ!"),
            ("–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞", "–ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", 3, "–°—Ä–µ–¥–Ω–µ, –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏")
        ]
        
        for query, answer, score, comment in feedback_examples:
            agent.learn_from_feedback(query, answer, score, comment)
            print(f"‚úÖ –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –∑–∞–ø–∏—Å–∞–Ω–∞ –¥–ª—è: '{query}' (–æ—Ü–µ–Ω–∫–∞: {score}/5)")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        analytics = agent.get_learning_analytics()
        
        if 'message' in analytics:
            print(f"\nüìà {analytics['message']}")
        else:
            print(f"\nüìà –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è:")
            print(f"  ‚Ä¢ –í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {analytics['total_feedback']}")
            print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {analytics['average_score']}/5")
            
            print(f"\nüìä –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤:")
            for query_type, stats in analytics['query_type_performance'].items():
                print(f"  ‚Ä¢ {query_type}: {stats['success_rate']:.1f}% —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ ({stats['success_count']}/{stats['total_queries']})")
            
            if analytics['recent_low_scores']:
                print(f"\n‚ö†Ô∏è  –ù–µ–¥–∞–≤–Ω–∏–µ –Ω–∏–∑–∫–∏–µ –æ—Ü–µ–Ω–∫–∏:")
                for feedback in analytics['recent_low_scores']:
                    print(f"  ‚Ä¢ '{feedback['query']}': {feedback['score']}/5 - {feedback['comments']}")
            
            if analytics['improvement_suggestions']:
                print(f"\nüí° –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é:")
                for suggestion in analytics['improvement_suggestions']:
                    print(f"  ‚Ä¢ {suggestion}")
                    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞–≥–µ–Ω—Ç–∞: {e}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω OPENAI_API_KEY")


def interactive_chat_demo():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç —Å RAG –∞–≥–µ–Ω—Ç–æ–º"""
    print("\n=== –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç —Å RAG –∞–≥–µ–Ω—Ç–æ–º (OpenRouter) ===")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á–∏
    openrouter_key = os.getenv("OPENROUTER_API_KEY")
    openai_key = os.getenv("OPENAI_API_KEY")
    
    if not openrouter_key and not openai_key:
        print("‚ö†Ô∏è  –ù–∏ OPENROUTER_API_KEY, –Ω–∏ OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã.")
        return
    
    # –í—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º OpenAI –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    use_openrouter = False  # bool(openrouter_key)
    provider_name = "OpenRouter" if use_openrouter else "OpenAI"
    print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider_name}")
    
    try:
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞...")
        config = AgentConfig(use_openrouter=use_openrouter)
        agent = MultiSourceRAGAgent(config)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑ –∑–Ω–∞–Ω–∏–π
        knowledge_base_path = os.path.join(os.path.dirname(__file__), "knowledge_base")
        
        if not os.path.exists(knowledge_base_path):
            print("‚ùå –ü–∞–ø–∫–∞ knowledge_base –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
        
        sources = [
            ("technical_docs", os.path.join(knowledge_base_path, "technical_docs"), 
             "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"),
            ("api_reference", os.path.join(knowledge_base_path, "api_reference"), 
             "API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"),
            ("tutorials", os.path.join(knowledge_base_path, "tutorials"), 
             "–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –∏ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã")
        ]
        
        for name, path, description in sources:
            if os.path.exists(path) and os.listdir(path):
                agent.add_knowledge_base(name, path, description)
        
        print("‚úÖ –ê–≥–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
        print("\nüí¨ –ù–∞—á–∏–Ω–∞–π—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã (–≤–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è)")
        print("–ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:")
        print("- –ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å FAISS –∏–Ω–¥–µ–∫—Å?")
        print("- –ß—Ç–æ —Ç–∞–∫–æ–µ RAG –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞?")
        print("- –ü–æ–∫–∞–∂–∏ –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ChromaDB")
        print("- –ö–∞–∫–∏–µ –µ—Å—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ RAG?")
        
        while True:
            try:
                user_input = input("\nü§î –í—ã: ").strip()
                
                if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit', 'q']:
                    print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    break
                
                if not user_input:
                    continue
                
                print("ü§ñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å...")
                response = agent.chat(user_input)
                print(f"ü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {response}")
                
            except KeyboardInterrupt:
                print("\n\nüëã –ß–∞—Ç –∑–∞–≤–µ—Ä—à–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                break
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ—Å—Å–∏–∏
        stats = agent.get_statistics()
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏:")
        print(f"- –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['conversation_turns']}")
        print(f"- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {stats['knowledge_bases']}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —á–∞—Ç–∞: {e}")


def comparison_demo():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è FAISS –∏ ChromaDB"""
    print("\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ FAISS –∏ ChromaDB ===")
    
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ö†Ô∏è  OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return
    
    try:
        from langchain_openai import OpenAIEmbeddings
        from langchain_community.vectorstores import FAISS, Chroma
        from langchain_community.document_loaders import DirectoryLoader
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        import time
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        knowledge_base_path = os.path.join(os.path.dirname(__file__), "knowledge_base")
        
        if not os.path.exists(knowledge_base_path):
            print("‚ùå –ü–∞–ø–∫–∞ knowledge_base –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
        
        print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
        loader = DirectoryLoader(knowledge_base_path, glob="**/*.md")
        documents = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(documents)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        openrouter_key = os.getenv("OPENROUTER_API_KEY")
        # –í—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º OpenAI –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        if False and openrouter_key:  # –û—Ç–∫–ª—é—á–∞–µ–º OpenRouter –≤—Ä–µ–º–µ–Ω–Ω–æ
            embeddings = OpenRouterEmbeddings(model="openai/text-embedding-ada-002")
            print("üîó –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenRouter –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        else:
            embeddings = OpenAIEmbeddings()
            print("üîó –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAI –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º FAISS
        print("\nüöÄ –¢–µ—Å—Ç–∏—Ä—É–µ–º FAISS...")
        start_time = time.time()
        faiss_store = FAISS.from_documents(chunks, embeddings)
        faiss_build_time = time.time() - start_time
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –≤ FAISS
        start_time = time.time()
        faiss_results = faiss_store.similarity_search("–≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö", k=3)
        faiss_search_time = time.time() - start_time
        
        print(f"‚è±Ô∏è  FAISS - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ: {faiss_build_time:.2f}—Å, –ü–æ–∏—Å–∫: {faiss_search_time:.3f}—Å")
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(faiss_results)}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º ChromaDB
        print("\nüöÄ –¢–µ—Å—Ç–∏—Ä—É–µ–º ChromaDB...")
        start_time = time.time()
        chroma_store = Chroma.from_documents(
            chunks, 
            embeddings,
            persist_directory="./demo_chroma_comparison",
            collection_name="comparison_test"
        )
        chroma_build_time = time.time() - start_time
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –≤ ChromaDB
        start_time = time.time()
        chroma_results = chroma_store.similarity_search("–≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö", k=3)
        chroma_search_time = time.time() - start_time
        
        print(f"‚è±Ô∏è  ChromaDB - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ: {chroma_build_time:.2f}—Å, –ü–æ–∏—Å–∫: {chroma_search_time:.3f}—Å")
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(chroma_results)}")
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ:")
        print(f"{'–ú–µ—Ç—Ä–∏–∫–∞':<20} {'FAISS':<15} {'ChromaDB':<15}")
        print("-" * 50)
        print(f"{'–í—Ä–µ–º—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è':<20} {faiss_build_time:.2f}—Å{'':<10} {chroma_build_time:.2f}—Å")
        print(f"{'–í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞':<20} {faiss_search_time:.3f}—Å{'':<10} {chroma_search_time:.3f}—Å")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\nüìù –ü—Ä–∏–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ FAISS:")
        if faiss_results:
            print(f"   {faiss_results[0].page_content[:150]}...")
        
        print(f"\nüìù –ü—Ä–∏–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ChromaDB:")
        if chroma_results:
            print(f"   {chroma_results[0].page_content[:150]}...")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏: {e}")


def demo_openrouter_models():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ OpenRouter"""
    import time
    
    print("\n=== –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π OpenRouter ===")
    
    if not os.getenv("OPENROUTER_API_KEY"):
        print("‚ö†Ô∏è  OPENROUTER_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
        print("üìù –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ API –≤—ã–∑–æ–≤–æ–≤:")
        
        models_info = [
            ("anthropic/claude-3.5-sonnet", "Claude 3.5 Sonnet", "–í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –æ—Ç–ª–∏—á–Ω–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á"),
            ("openai/gpt-4o-mini", "GPT-4o Mini", "–ë—ã—Å—Ç—Ä–∞—è –∏ –¥–µ—à–µ–≤–∞—è –º–æ–¥–µ–ª—å OpenAI"),
            ("meta-llama/llama-3.1-8b-instruct", "Llama 3.1 8B", "Open source, —á–∞—Å—Ç–æ –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è"),
            ("google/gemini-flash-1.5", "Gemini Flash", "–ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å Google"),
            ("mistralai/mistral-7b-instruct", "Mistral 7B", "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –µ–≤—Ä–æ–ø–µ–π—Å–∫–∞—è –º–æ–¥–µ–ª—å"),
            ("anthropic/claude-3-haiku", "Claude 3 Haiku", "–°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å Anthropic")
        ]
        
        print("\nü§ñ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ OpenRouter:")
        for model_id, name, description in models_info:
            print(f"   ‚Ä¢ {name}")
            print(f"     ID: {model_id}")
            print(f"     –û–ø–∏—Å–∞–Ω–∏–µ: {description}")
            print()
        
        print("üí° –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OPENROUTER_API_KEY –≤ .env —Ñ–∞–π–ª–µ")
        return
    
    # –°–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π OpenRouter
    models_to_test = [
        ("anthropic/claude-3.5-sonnet", "Claude 3.5 Sonnet - –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ"),
        ("openai/gpt-4o-mini", "GPT-4o Mini - –ë—ã—Å—Ç—Ä–∞—è –∏ –¥–µ—à–µ–≤–∞—è"),
        ("meta-llama/llama-3.1-8b-instruct", "Llama 3.1 8B - Open source"),
        ("google/gemini-flash-1.5", "Gemini Flash - –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å Google")
    ]
    
    test_query = "–ß—Ç–æ —Ç–∞–∫–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö?"
    
    for model_name, description in models_to_test:
        print(f"\nü§ñ –¢–µ—Å—Ç–∏—Ä—É–µ–º: {description}")
        print(f"   –ú–æ–¥–µ–ª—å: {model_name}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
            config = AgentConfig(
                model_name=model_name,
                use_openrouter=True,
                max_tokens=500  # –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è –¥–µ–º–æ
            )
            
            # –°–æ–∑–¥–∞–µ–º LLM –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            llm = ChatOpenRouter(
                model_name=model_name,
                temperature=0.1,
                max_tokens=500,
                request_timeout=20,
                max_retries=1
            )
            
            start_time = time.time()
            response = llm.invoke(f"–ö—Ä–∞—Ç–∫–æ –æ–±—ä—è—Å–Ω–∏: {test_query}").content
            elapsed_time = time.time() - start_time
            
            print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {elapsed_time:.1f}—Å")
            print(f"   üìù –û—Ç–≤–µ—Ç: {response[:150]}...")
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        
        print("-" * 50)


if __name__ == "__main__":
    import sys
    
    print("üöÄ RAG Agent Examples - –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    print("=" * 60)
    
    if len(sys.argv) > 1:
        mode = sys.argv[1].lower()
        if mode == 'chat':
            interactive_chat_demo()
        elif mode == 'compare':
            comparison_demo()
        elif mode == 'multi':
            demo_multi_source_agent()
        elif mode == 'smart':
            demo_smart_retrieval_agent()
        elif mode == 'models':
            demo_openrouter_models()
        else:
            print(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∂–∏–º: {mode}")
            print("–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ–∂–∏–º—ã: chat, compare, multi, smart, models")
    else:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –¥–µ–º–æ –ø–æ –æ—á–µ—Ä–µ–¥–∏
        demo_multi_source_agent()
        demo_smart_retrieval_agent()
        
        print("\n" + "=" * 60)
        print("‚ú® –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∂–∏–º—ã:")
        print("  python rag_agent_examples.py chat     - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç")
        print("  python rag_agent_examples.py compare  - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ FAISS vs ChromaDB")
        print("  python rag_agent_examples.py multi    - –¢–æ–ª—å–∫–æ MultiSource –¥–µ–º–æ")
        print("  python rag_agent_examples.py smart    - –¢–æ–ª—å–∫–æ Smart Retrieval –¥–µ–º–æ")
        print("  python rag_agent_examples.py models   - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π OpenRouter")
    
    print("\n=== –ó–∞–∫–ª—é—á–µ–Ω–∏–µ ===")
    print("–ü—Ä–∏–º–µ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç:")
    print("1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é RAG —Å –∞–≥–µ–Ω—Ç–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º")
    print("2. –†–∞–±–æ—Ç—É —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –∑–Ω–∞–Ω–∏–π")
    print("3. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é")
    print("4. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏")
    print("5. –ê–Ω–∞–ª–∏—Ç–∏–∫—É –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞")
    print("6. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")